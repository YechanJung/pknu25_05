{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "data_dir = os.getenv(\"DATA_DIR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp4_noise_reducer import noise_reduce_mp4\n",
    "\n",
    "enhanced, sr = noise_reduce_mp4(data_dir+\"/train/train_splits/dia0_utt3.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intensity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import scipy.signal  \n",
    "\n",
    "def compute_linear_trend(feature_sequence):\n",
    "    \"\"\"\n",
    "    Calculate the slope of the linear trend in a feature sequence.\n",
    "    Positive values indicate rising trend, negative values indicate falling trend.\n",
    "    \n",
    "    Args:\n",
    "        feature_sequence: Array of feature values over time\n",
    "        \n",
    "    Returns:\n",
    "        Slope of the best-fit line\n",
    "\n",
    "    Reference:\n",
    "\n",
    "    Schuller, B., Steidl, S., Batliner, A., Burkhardt, F., Devillers, L., Müller, C., & Narayanan, S. (2013). \n",
    "        Paralinguistics in speech and language—State-of-the-art and the challenge. Computer Speech & Language, 27(1), 4-39.\n",
    "        Shows linear regression coefficients of energy contours as key predictors of emotion intensity.\n",
    "\n",
    "    Batliner, A., Steidl, S., Schuller, B., Seppi, D., Vogt, T., Wagner, J., ... & Amir, N. (2011).\n",
    "        Whodunnit-searching for the most important feature types signalling emotion-related user states in speech.\n",
    "        Computer Speech & Language, 25(1), 4-28. Validates linear trend features for emotion intensity prediction.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create time indices (normalized to [0,1])\n",
    "    feature_sequence = np.array(feature_sequence)  # Ensure numpy array\n",
    "    n_frames = len(feature_sequence)\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if n_frames < 2:\n",
    "        return 0.0\n",
    "    \n",
    "    # Check if array contains only identical values\n",
    "    if np.all(feature_sequence == feature_sequence[0]):\n",
    "        return 0.0\n",
    "    \n",
    "    time_indices = np.linspace(0, 1, n_frames)\n",
    "    \n",
    "    # Compute linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(\n",
    "        time_indices, feature_sequence\n",
    "    )\n",
    "    \n",
    "    # Normalize by feature range to handle different scales\n",
    "    feature_range = np.max(feature_sequence) - np.min(feature_sequence)\n",
    "    if feature_range > 0:\n",
    "        normalized_slope = slope / feature_range\n",
    "    else:\n",
    "        normalized_slope = 0.0\n",
    "    \n",
    "    return normalized_slope\n",
    "\n",
    "def compute_feature_dynamism(feature_sequence):\n",
    "    \"\"\"\n",
    "    Calculate the dynamism (variability in rate of change) of a feature.\n",
    "    Higher values indicate more dynamic expression.\n",
    "    \n",
    "    Args:\n",
    "        feature_sequence: Array of feature values over time\n",
    "        \n",
    "    Returns:\n",
    "        Dynamism score\n",
    "\n",
    "    References:\n",
    "    \n",
    "    Schuller, B., Batliner, A., Steidl, S., & Seppi, D. (2011). \n",
    "        Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge.Speech Communication, 53(9-10), 1062-1087.\n",
    "        Demonstrates that first and second derivatives of prosodic features significantly improve emotion intensity recognition.\n",
    "\n",
    "    Eyben, F., Scherer, K. R., Schuller, B. W., Sundberg, J., André, E., Busso, C., ... & Truong, K. P. (2016). \n",
    "        The Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing. IEEE Transactions on Affective Computing, 7(2), 190-202. \n",
    "        Defines dynamism features as key performance indicators for emotion intensity detection.\n",
    "    \"\"\"\n",
    "    feature_sequence = np.array(feature_sequence)  # Ensure numpy array\n",
    "    \n",
    "    # Handle edge cases\n",
    "    if len(feature_sequence) < 3:  # Need at least 3 points for second derivative\n",
    "        print (\"not enough len\")\n",
    "        return 0.0\n",
    "    \n",
    "    # Compute first derivative (rate of change)\n",
    "    first_derivative = np.diff(feature_sequence)\n",
    "    \n",
    "    # Compute second derivative (acceleration)\n",
    "    second_derivative = np.diff(first_derivative)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    first_derivative_mean = np.mean(np.abs(first_derivative))\n",
    "    first_derivative_std = np.std(first_derivative)\n",
    "    second_derivative_mean = np.mean(np.abs(second_derivative))\n",
    "    \n",
    "    # Combine into dynamism measure (normalized to feature range)\n",
    "    feature_range = np.max(feature_sequence) - np.min(feature_sequence)\n",
    "    if feature_range > 0:\n",
    "        dynamism = (0.5 * first_derivative_std + \n",
    "                   0.3 * first_derivative_mean + \n",
    "                   0.2 * second_derivative_mean) / feature_range\n",
    "    else:\n",
    "        dynamism = 0.0\n",
    "    \n",
    "    return dynamism\n",
    "\n",
    "def detect_bursts(feature_sequence, threshold=1.5, min_duration=3):\n",
    "    \"\"\"\n",
    "    Detect burst patterns in a feature sequence.\n",
    "    \n",
    "    Args:\n",
    "        feature_sequence: Array of feature values over time\n",
    "        threshold: Threshold for burst detection (multiplier over local mean)\n",
    "        min_duration: Minimum frames to be considered a burst\n",
    "        \n",
    "    Returns:\n",
    "        List of (start_index, end_index, magnitude) for each burst\n",
    "    \"\"\"\n",
    "    feature_sequence = np.array(feature_sequence)  # Ensure numpy array\n",
    "    \n",
    "    # Handle empty array\n",
    "    if len(feature_sequence) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Normalize feature sequence\n",
    "    feature_mean = np.mean(feature_sequence)\n",
    "    feature_std = np.std(feature_sequence)\n",
    "    if feature_std > 0:\n",
    "        normalized_sequence = (feature_sequence - feature_mean) / feature_std\n",
    "    else:\n",
    "        return []\n",
    "    \n",
    "    # Detect regions above threshold\n",
    "    above_threshold = normalized_sequence > threshold\n",
    "    \n",
    "    # Find continuous regions (bursts)\n",
    "    bursts = []\n",
    "    in_burst = False\n",
    "    burst_start = 0\n",
    "    \n",
    "    for i, is_above in enumerate(above_threshold):\n",
    "        if is_above and not in_burst:\n",
    "            # Start of new burst\n",
    "            in_burst = True\n",
    "            burst_start = i\n",
    "        elif not is_above and in_burst:\n",
    "            # End of burst\n",
    "            burst_end = i\n",
    "            burst_duration = burst_end - burst_start\n",
    "            \n",
    "            if burst_duration >= min_duration:\n",
    "                # Calculate burst magnitude\n",
    "                burst_magnitude = np.mean(normalized_sequence[burst_start:burst_end])\n",
    "                bursts.append((burst_start, burst_end, burst_magnitude))\n",
    "            \n",
    "            in_burst = False\n",
    "    \n",
    "    # Handle case where sequence ends during a burst\n",
    "    if in_burst:\n",
    "        burst_end = len(above_threshold)\n",
    "        burst_duration = burst_end - burst_start\n",
    "        \n",
    "        if burst_duration >= min_duration:\n",
    "            burst_magnitude = np.mean(normalized_sequence[burst_start:burst_end])\n",
    "            bursts.append((burst_start, burst_end, burst_magnitude))\n",
    "    \n",
    "    return bursts\n",
    "\n",
    "def compute_burst_features(feature_sequence, threshold=1.5, min_duration=3):\n",
    "    \"\"\"\n",
    "    Compute features based on burst patterns.\n",
    "    \n",
    "    Args:\n",
    "        feature_sequence: Array of feature values over time\n",
    "        threshold: Threshold for burst detection\n",
    "        min_duration: Minimum frames for a burst\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of burst-related features\n",
    "    \"\"\"\n",
    "    feature_sequence = np.array(feature_sequence)  # Ensure numpy array\n",
    "    \n",
    "    bursts = detect_bursts(feature_sequence, threshold, min_duration)\n",
    "    \n",
    "    n_frames = len(feature_sequence)\n",
    "    if n_frames == 0:\n",
    "        return {\n",
    "            'burst_count': 0,\n",
    "            'burst_rate': 0,\n",
    "            'mean_burst_magnitude': 0,\n",
    "            'max_burst_magnitude': 0,\n",
    "            'burst_coverage': 0,\n",
    "            'first_burst_position': 0\n",
    "        }\n",
    "    \n",
    "    # Compute burst features\n",
    "    burst_count = len(bursts)\n",
    "    burst_rate = burst_count / n_frames\n",
    "    \n",
    "    if burst_count > 0:\n",
    "        burst_magnitudes = [b[2] for b in bursts]\n",
    "        burst_durations = [b[1] - b[0] for b in bursts]\n",
    "        total_burst_frames = sum(burst_durations)\n",
    "        \n",
    "        mean_burst_magnitude = np.mean(burst_magnitudes)\n",
    "        max_burst_magnitude = np.max(burst_magnitudes)\n",
    "        burst_coverage = total_burst_frames / n_frames\n",
    "        first_burst_position = bursts[0][0] / n_frames if bursts else 0\n",
    "    else:\n",
    "        mean_burst_magnitude = 0\n",
    "        max_burst_magnitude = 0\n",
    "        burst_coverage = 0\n",
    "        first_burst_position = 0\n",
    "    \n",
    "    return {\n",
    "        'burst_count': burst_count,\n",
    "        'burst_rate': burst_rate,\n",
    "        'mean_burst_magnitude': mean_burst_magnitude,\n",
    "        'max_burst_magnitude': max_burst_magnitude,\n",
    "        'burst_coverage': burst_coverage,\n",
    "        'first_burst_position': first_burst_position\n",
    "    }\n",
    "\n",
    "def analyze_peaks(feature_sequence, distance=5, prominence=0.1, width=None):\n",
    "    \"\"\"\n",
    "    Perform comprehensive peak analysis on a feature sequence.\n",
    "    \n",
    "    Args:\n",
    "        feature_sequence: Array of feature values over time\n",
    "        distance: Minimum samples between peaks\n",
    "        prominence: Minimum peak prominence\n",
    "        width: Minimum peak width\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of peak-related features\n",
    "    \"\"\"\n",
    "    feature_sequence = np.array(feature_sequence)  # Ensure numpy array\n",
    "    \n",
    "    # Handle empty array\n",
    "    n_frames = len(feature_sequence)\n",
    "    if n_frames == 0:\n",
    "        return {\n",
    "            'peak_count': 0,\n",
    "            'peak_rate': 0,\n",
    "            'mean_peak_prominence': 0,\n",
    "            'max_peak_prominence': 0,\n",
    "            'peak_position_mean': 0.5,\n",
    "            'peak_position_std': 0,\n",
    "            'peak_width_mean': 0\n",
    "        }\n",
    "    \n",
    "    # Normalize feature sequence to [0,1]\n",
    "    if np.max(feature_sequence) > np.min(feature_sequence):\n",
    "        normalized = (feature_sequence - np.min(feature_sequence)) / (np.max(feature_sequence) - np.min(feature_sequence))\n",
    "    else:\n",
    "        normalized = np.zeros_like(feature_sequence)\n",
    "    \n",
    "    # Find peaks with try/except to handle potential errors\n",
    "    try:\n",
    "        peaks, properties = scipy.signal.find_peaks(\n",
    "            normalized,\n",
    "            distance=distance,\n",
    "            prominence=prominence,\n",
    "            width=width\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding peaks: {e}\")\n",
    "        return {\n",
    "            'peak_count': 0,\n",
    "            'peak_rate': 0,\n",
    "            'mean_peak_prominence': 0,\n",
    "            'max_peak_prominence': 0,\n",
    "            'peak_position_mean': 0.5,\n",
    "            'peak_position_std': 0,\n",
    "            'peak_width_mean': 0\n",
    "        }\n",
    "    \n",
    "    # Compute peak features\n",
    "    peak_count = len(peaks)\n",
    "    peak_rate = peak_count / n_frames\n",
    "    \n",
    "    if peak_count > 0:\n",
    "        # Make sure properties contains all expected keys\n",
    "        peak_prominences = properties.get('prominences', np.zeros(peak_count))\n",
    "        peak_widths = properties.get('widths', np.zeros(peak_count))\n",
    "        peak_positions = peaks / n_frames  # Normalize positions to [0,1]\n",
    "        \n",
    "        mean_peak_prominence = np.mean(peak_prominences)\n",
    "        max_peak_prominence = np.max(peak_prominences) if len(peak_prominences) > 0 else 0\n",
    "        peak_position_mean = np.mean(peak_positions)\n",
    "        peak_position_std = np.std(peak_positions)\n",
    "        peak_width_mean = np.mean(peak_widths) / n_frames if len(peak_widths) > 0 else 0  # Normalize to utterance length\n",
    "    else:\n",
    "        mean_peak_prominence = 0\n",
    "        max_peak_prominence = 0\n",
    "        peak_position_mean = 0.5  # Default to middle\n",
    "        peak_position_std = 0\n",
    "        peak_width_mean = 0\n",
    "    \n",
    "    return {\n",
    "        'peak_count': peak_count,\n",
    "        'peak_rate': peak_rate,\n",
    "        'mean_peak_prominence': mean_peak_prominence,\n",
    "        'max_peak_prominence': max_peak_prominence,\n",
    "        'peak_position_mean': peak_position_mean,\n",
    "        'peak_position_std': peak_position_std,\n",
    "        'peak_width_mean': peak_width_mean\n",
    "    }\n",
    "\n",
    "def extract_energy_envelope(audio_signal, sample_rate=16000, window_size=0.025, window_step=0.010):\n",
    "    \"\"\"\n",
    "    Extract energy envelope from audio signal.\n",
    "    \n",
    "    Args:\n",
    "        audio_signal: Raw audio samples\n",
    "        sample_rate: Sample rate in Hz\n",
    "        window_size: Window size in seconds\n",
    "        window_step: Window step in seconds\n",
    "        \n",
    "    Returns:\n",
    "        Energy envelope (one value per frame)\n",
    "    \"\"\"\n",
    "    audio_signal = np.array(audio_signal)  # Ensure numpy array\n",
    "    \n",
    "    # Handle empty array\n",
    "    if len(audio_signal) == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Ensure audio is 1D\n",
    "    if len(audio_signal.shape) > 1:\n",
    "        audio_signal = audio_signal.flatten()\n",
    "    \n",
    "    # Convert window sizes from seconds to samples\n",
    "    window_length = int(window_size * sample_rate)\n",
    "    hop_length = int(window_step * sample_rate)\n",
    "    \n",
    "    # Ensure valid window and hop lengths\n",
    "    window_length = max(window_length, 1)\n",
    "    hop_length = max(hop_length, 1)\n",
    "    \n",
    "    try:\n",
    "        # Compute RMS energy in each frame\n",
    "        energy = librosa.feature.rms(\n",
    "            y=audio_signal,\n",
    "            frame_length=window_length,\n",
    "            hop_length=hop_length\n",
    "        )[0]\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting energy envelope: {e}\")\n",
    "        return np.array([])\n",
    "    \n",
    "    return energy\n",
    "\n",
    "def compute_envelope_features(envelope):\n",
    "    \"\"\"\n",
    "    Compute features from energy envelope.\n",
    "    \n",
    "    Args:\n",
    "        envelope: Energy envelope (one value per frame)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of envelope features\n",
    "    \"\"\"\n",
    "    envelope = np.array(envelope)  # Ensure numpy array\n",
    "    \n",
    "    # Handle empty array\n",
    "    if len(envelope) < 2:\n",
    "        return {\n",
    "            'mean_attack_rate': 0,\n",
    "            'mean_decay_rate': 0,\n",
    "            'attack_decay_ratio': 0,\n",
    "            'envelope_modulation': 0,\n",
    "            'attack_count': 0,\n",
    "            'decay_count': 0\n",
    "        }\n",
    "    \n",
    "    # Normalize envelope\n",
    "    if np.max(envelope) > np.min(envelope):\n",
    "        normalized = (envelope - np.min(envelope)) / (np.max(envelope) - np.min(envelope))\n",
    "    else:\n",
    "        normalized = np.zeros_like(envelope)\n",
    "    \n",
    "    # Compute attack and decay\n",
    "    attack_rates = []\n",
    "    decay_rates = []\n",
    "    \n",
    "    # Find regions of increasing energy (attacks) and decreasing energy (decays)\n",
    "    in_attack = False\n",
    "    in_decay = False\n",
    "    attack_start = 0\n",
    "    decay_start = 0\n",
    "    \n",
    "    for i in range(1, len(normalized)):\n",
    "        # Check for attack (significant increase)\n",
    "        if normalized[i] > normalized[i-1] + 0.05:\n",
    "            if not in_attack:\n",
    "                in_attack = True\n",
    "                attack_start = i - 1\n",
    "            in_decay = False\n",
    "        # Check for decay (significant decrease)\n",
    "        elif normalized[i] < normalized[i-1] - 0.05:\n",
    "            if in_attack:\n",
    "                # End of attack, calculate rate\n",
    "                attack_duration = i - attack_start\n",
    "                attack_magnitude = normalized[i-1] - normalized[attack_start]\n",
    "                if attack_duration > 0:\n",
    "                    attack_rates.append(attack_magnitude / attack_duration)\n",
    "                in_attack = False\n",
    "            \n",
    "            if not in_decay:\n",
    "                in_decay = True\n",
    "                decay_start = i - 1\n",
    "        # Check for end of decay\n",
    "        elif in_decay and (normalized[i] >= normalized[i-1] or i == len(normalized) - 1):\n",
    "            # End of decay, calculate rate\n",
    "            decay_duration = i - decay_start\n",
    "            decay_magnitude = normalized[decay_start] - normalized[i-1]\n",
    "            if decay_duration > 0:\n",
    "                decay_rates.append(decay_magnitude / decay_duration)\n",
    "            in_decay = False\n",
    "    \n",
    "    # Compute summary features\n",
    "    mean_attack_rate = np.mean(attack_rates) if attack_rates else 0\n",
    "    mean_decay_rate = np.mean(decay_rates) if decay_rates else 0\n",
    "    attack_decay_ratio = mean_attack_rate / mean_decay_rate if mean_decay_rate > 0 else 0\n",
    "    \n",
    "    # Compute envelope modulation\n",
    "    modulation = np.std(np.diff(normalized)) if len(normalized) > 1 else 0\n",
    "    \n",
    "    return {\n",
    "        'mean_attack_rate': mean_attack_rate,\n",
    "        'mean_decay_rate': mean_decay_rate,\n",
    "        'attack_decay_ratio': attack_decay_ratio,\n",
    "        'envelope_modulation': modulation,\n",
    "        'attack_count': len(attack_rates),\n",
    "        'decay_count': len(decay_rates)\n",
    "    }\n",
    "\n",
    "def extract_complete_trajectory_features(utterance, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Extract comprehensive trajectory-based features from a speech utterance.\n",
    "    \n",
    "    Args:\n",
    "        utterance: Audio samples\n",
    "        sample_rate: Sample rate in Hz\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of trajectory features\n",
    "    \"\"\"\n",
    "    utterance = np.array(utterance)  # Ensure numpy array\n",
    "    \n",
    "    # Handle empty utterance\n",
    "    if len(utterance) == 0:\n",
    "        return {'error': 'Empty utterance'}\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    try:\n",
    "        # Extract energy envelope\n",
    "        energy_envelope = extract_energy_envelope(utterance, sample_rate)\n",
    "        \n",
    "        # Skip if energy envelope extraction failed\n",
    "        if len(energy_envelope) == 0:\n",
    "            return {'error': 'Failed to extract energy envelope'}\n",
    "        \n",
    "        # Extract F0 contour (pitch) with error handling\n",
    "        try:\n",
    "            f0, voiced_flag, _ = librosa.pyin(\n",
    "                y=utterance,\n",
    "                fmin=librosa.note_to_hz('C2'),\n",
    "                fmax=librosa.note_to_hz('C7'),\n",
    "                sr=sample_rate,\n",
    "                frame_length=2048,\n",
    "                hop_length=512\n",
    "            )\n",
    "            # Replace NaN values with zeros for unvoiced frames\n",
    "            f0 = np.nan_to_num(f0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting pitch: {e}\")\n",
    "            f0 = np.zeros(len(energy_envelope))\n",
    "        \n",
    "        # 1. Linear trends\n",
    "        features['energy_trend'] = compute_linear_trend(energy_envelope)\n",
    "        features['f0_trend'] = compute_linear_trend(f0[f0 > 0]) if np.any(f0 > 0) else 0\n",
    "        \n",
    "        # 2. Dynamism features\n",
    "        features['energy_dynamism'] = compute_feature_dynamism(energy_envelope)\n",
    "        features['f0_dynamism'] = compute_feature_dynamism(f0[f0 > 0]) if np.any(f0 > 0) else 0\n",
    "        \n",
    "        # 3. Burst features\n",
    "        energy_burst_features = compute_burst_features(energy_envelope)\n",
    "        features.update({f'energy_{k}': v for k, v in energy_burst_features.items()})\n",
    "        \n",
    "        # 4. Peak analysis\n",
    "        energy_peak_features = analyze_peaks(energy_envelope)\n",
    "        features.update({f'energy_{k}': v for k, v in energy_peak_features.items()})\n",
    "        \n",
    "        # 5. Envelope features\n",
    "        envelope_features = compute_envelope_features(energy_envelope)\n",
    "        features.update(envelope_features)\n",
    "        \n",
    "        # 6. Combined joy-specific features\n",
    "        # Joy often has quick attacks, multiple peaks, and rising contours\n",
    "        features['joy_signature'] = (\n",
    "            0.3 * features['energy_trend'] +\n",
    "            0.2 * features.get('energy_burst_rate', 0) +\n",
    "            0.2 * features['mean_attack_rate'] +\n",
    "            0.15 * features.get('energy_peak_rate', 0) +\n",
    "            0.15 * features['energy_dynamism']\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting trajectory features: {e}\")\n",
    "        return {'error': str(e)}\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'energy_trend': -0.2923579637599906,\n",
       " 'f0_trend': -0.9580067027920092,\n",
       " 'energy_dynamism': 0.06232120166007513,\n",
       " 'f0_dynamism': 0.07913862407492742,\n",
       " 'energy_burst_count': 6,\n",
       " 'energy_burst_rate': 0.02158273381294964,\n",
       " 'energy_mean_burst_magnitude': 1.8132095,\n",
       " 'energy_max_burst_magnitude': 1.9318947,\n",
       " 'energy_burst_coverage': 0.1223021582733813,\n",
       " 'energy_first_burst_position': 0.12589928057553956,\n",
       " 'energy_peak_count': 13,\n",
       " 'energy_peak_rate': 0.046762589928057555,\n",
       " 'energy_mean_peak_prominence': 0.5071544614656887,\n",
       " 'energy_max_peak_prominence': 0.9999633678708051,\n",
       " 'energy_peak_position_mean': 0.44770337576092967,\n",
       " 'energy_peak_position_std': 0.2040487080150044,\n",
       " 'energy_peak_width_mean': 0.0,\n",
       " 'mean_attack_rate': 0.0640265149483969,\n",
       " 'mean_decay_rate': 0.04808661511966161,\n",
       " 'attack_decay_ratio': 1.3314830912733093,\n",
       " 'envelope_modulation': 0.07911677,\n",
       " 'attack_count': 13,\n",
       " 'decay_count': 6,\n",
       " 'joy_signature': -0.05422297063750797}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = extract_complete_trajectory_features(enhanced)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.086893652284115"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['joy_signature']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp4_noise_reducer import noise_reduce_mp4\n",
    "enhanced, sr = noise_reduce_mp4(data_dir+\"/train/train_splits/dia2_utt10.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'energy_trend': 0.35661042034150153,\n",
       " 'f0_trend': 0,\n",
       " 'energy_dynamism': 0.07870307468648524,\n",
       " 'f0_dynamism': 0,\n",
       " 'energy_burst_count': 2,\n",
       " 'energy_burst_rate': 0.024390243902439025,\n",
       " 'energy_mean_burst_magnitude': 2.0588627,\n",
       " 'energy_max_burst_magnitude': 2.0838885,\n",
       " 'energy_burst_coverage': 0.13414634146341464,\n",
       " 'energy_first_burst_position': 0.5121951219512195,\n",
       " 'energy_peak_count': 4,\n",
       " 'energy_peak_rate': 0.04878048780487805,\n",
       " 'energy_mean_peak_prominence': 0.580458188573175,\n",
       " 'energy_max_peak_prominence': 0.99991062340996,\n",
       " 'energy_peak_position_mean': 0.6067073170731707,\n",
       " 'energy_peak_position_std': 0.16164287977818978,\n",
       " 'energy_peak_width_mean': 0.0,\n",
       " 'mean_attack_rate': 0.09760881179854983,\n",
       " 'mean_decay_rate': 0.04873283704121907,\n",
       " 'attack_decay_ratio': 2.002937192349188,\n",
       " 'envelope_modulation': 0.103842385,\n",
       " 'attack_count': 4,\n",
       " 'decay_count': 2,\n",
       " 'joy_signature': 0.15050547161635275}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = extract_complete_trajectory_features(enhanced)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced, sr = noise_reduce_mp4(data_dir+\"/train/train_splits/dia2_utt2.mp4\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'energy_trend': -0.0780977017880681,\n",
       " 'f0_trend': 0,\n",
       " 'energy_dynamism': 0.05008693664917137,\n",
       " 'f0_dynamism': 0,\n",
       " 'energy_burst_count': 4,\n",
       " 'energy_burst_rate': 0.021739130434782608,\n",
       " 'energy_mean_burst_magnitude': 2.2071576,\n",
       " 'energy_max_burst_magnitude': 2.748401,\n",
       " 'energy_burst_coverage': 0.11413043478260869,\n",
       " 'energy_first_burst_position': 0.28804347826086957,\n",
       " 'energy_peak_count': 5,\n",
       " 'energy_peak_rate': 0.02717391304347826,\n",
       " 'energy_mean_peak_prominence': 0.56592720043991,\n",
       " 'energy_max_peak_prominence': 0.9999731487550889,\n",
       " 'energy_peak_position_mean': 0.4141304347826087,\n",
       " 'energy_peak_position_std': 0.17891581697887246,\n",
       " 'energy_peak_width_mean': 0.0,\n",
       " 'mean_attack_rate': 0.10134079683233391,\n",
       " 'mean_decay_rate': 0.06733547932232208,\n",
       " 'attack_decay_ratio': 1.5050133726268566,\n",
       " 'envelope_modulation': 0.068232305,\n",
       " 'attack_count': 4,\n",
       " 'decay_count': 6,\n",
       " 'joy_signature': 0.012775802370900317}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = extract_complete_trajectory_features(enhanced)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced, sr = noise_reduce_mp4(data_dir+\"/train/train_splits/dia4_utt8.mp4\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'energy_trend': 0.08787549531940828,\n",
       " 'f0_trend': -0.37950716873878626,\n",
       " 'energy_dynamism': 0.05043121766442448,\n",
       " 'f0_dynamism': 0.04996396526066853,\n",
       " 'energy_burst_count': 6,\n",
       " 'energy_burst_rate': 0.007434944237918215,\n",
       " 'energy_mean_burst_magnitude': 2.0978127,\n",
       " 'energy_max_burst_magnitude': 2.5563905,\n",
       " 'energy_burst_coverage': 0.06815365551425032,\n",
       " 'energy_first_burst_position': 0.07187112763320942,\n",
       " 'energy_peak_count': 41,\n",
       " 'energy_peak_rate': 0.05080545229244114,\n",
       " 'energy_mean_peak_prominence': 0.38851035728932803,\n",
       " 'energy_max_peak_prominence': 0.9862197004258633,\n",
       " 'energy_peak_position_mean': 0.513252939220842,\n",
       " 'energy_peak_position_std': 0.2830363448923257,\n",
       " 'energy_peak_width_mean': 0.0,\n",
       " 'mean_attack_rate': 0.04114995544902342,\n",
       " 'mean_decay_rate': 0.048235711884666604,\n",
       " 'attack_decay_ratio': 0.8531014437480368,\n",
       " 'envelope_modulation': 0.061952095,\n",
       " 'attack_count': 38,\n",
       " 'decay_count': 40,\n",
       " 'joy_signature': 0.05126512902674066}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = extract_complete_trajectory_features(enhanced)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
