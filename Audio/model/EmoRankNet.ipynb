{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import Wav2Vec2Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankNetLoss(nn.Module):\n",
    "    def __init__(self, sigma=1.0):\n",
    "        super(RankNetLoss, self).__init__()\n",
    "        self.sigma = sigma\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "\n",
    "    def forward(self, o_i, o_j, P_ij):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            o_i (torch.Tensor): Model's raw score for item i (batch_size x 1)\n",
    "            o_j (torch.Tensor): Model's raw score for item j (batch_size x 1)\n",
    "            P_ij (torch.Tensor): True probability that item i is ranked higher than item j (batch_size x 1)\n",
    "                                 P_ij = 1.0 if score_i > score_j\n",
    "                                 P_ij = 0.0 if score_i < score_j\n",
    "                                 P_ij = 0.5 if score_i == score_j\n",
    "        \"\"\"\n",
    "        # Difference in scores\n",
    "        s_ij = o_i - o_j\n",
    "        \n",
    "        # Predicted probability that item i is ranked higher than item j\n",
    "        # P_hat_ij = torch.sigmoid(self.sigma * s_ij) # Original RankNet often uses just sigmoid(s_ij)\n",
    "        P_hat_ij = torch.sigmoid(s_ij / self.sigma) # Scaling by sigma, sometimes sigma is learned or fixed.\n",
    "                                                  # Using 1/sigma here to match some conventions where sigma acts like a temperature.\n",
    "                                                  # If sigma=1, it's just torch.sigmoid(s_ij)\n",
    "        \n",
    "        # Ensure P_ij is on the same device and has the correct shape\n",
    "        P_ij = P_ij.to(P_hat_ij.device).float().view_as(P_hat_ij)\n",
    "\n",
    "        loss = self.bce_loss(P_hat_ij, P_ij)\n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairwiseIntensityDataset(Dataset):\n",
    "    def __init__(self, audio_input_list, intensity_scores, processor, target_sampling_rate=16000, num_pairs_per_epoch=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            audio_input_list (list): List of audio file paths or pre-loaded audio tensors.\n",
    "                                     For simplicity, this example will assume pre-loaded tensors or arrays.\n",
    "            intensity_scores (np.array or torch.Tensor): Array of derived intensity scores.\n",
    "            processor (Wav2Vec2Processor): Wav2Vec2 processor for audio tokenization.\n",
    "            target_sampling_rate (int): Sampling rate Wav2Vec2 expects.\n",
    "            num_pairs_per_epoch (int, optional): Number of pairs to generate per epoch.\n",
    "        \"\"\"\n",
    "        self.audio_input_list = audio_input_list # This should be your actual audio data\n",
    "        self.intensity_scores = torch.tensor(intensity_scores, dtype=torch.float32)\n",
    "        self.processor = processor\n",
    "        self.target_sampling_rate = target_sampling_rate\n",
    "        self.num_samples = len(self.audio_input_list)\n",
    "        \n",
    "        self.pairs = []\n",
    "        # For simplicity, we'll sample pairs on the fly in __getitem__ if num_pairs_per_epoch is set.\n",
    "        # If num_pairs_per_epoch is None, you might want to pre-generate all pairs if memory allows.\n",
    "        if num_pairs_per_epoch is None and self.num_samples > 0 : # Pre-generate if few samples\n",
    "            for i in range(self.num_samples):\n",
    "                for j in range(self.num_samples):\n",
    "                    if i == j: continue\n",
    "                    self.pairs.append((i,j))\n",
    "        self.num_pairs_per_epoch = num_pairs_per_epoch\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.num_pairs_per_epoch is not None:\n",
    "            return self.num_pairs_per_epoch\n",
    "        return len(self.pairs) if self.pairs else self.num_samples * (self.num_samples -1) if self.num_samples > 1 else 0\n",
    "\n",
    "\n",
    "    def _process_audio(self, audio_input):\n",
    "        # This function would load audio if paths are given, resample, and process.\n",
    "        # For this example, let's assume audio_input is already a suitable waveform (e.g., numpy array).\n",
    "        # If it's a path, you'd load it here using librosa or similar.\n",
    "        # Example:\n",
    "        # if isinstance(audio_input, str):\n",
    "        #     waveform, sr = librosa.load(audio_input, sr=self.target_sampling_rate)\n",
    "        # else:\n",
    "        #     waveform = audio_input # Assuming it's already a waveform\n",
    "\n",
    "        # For dummy data, we'll just use the input directly if it's a tensor\n",
    "        processed = self.processor(audio_input, sampling_rate=self.target_sampling_rate, return_tensors=\"pt\", padding=True)\n",
    "        return processed.input_values.squeeze(0) # Remove batch dim if processor adds it for single sample\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.num_pairs_per_epoch is not None:\n",
    "            i = random.randint(0, self.num_samples - 1)\n",
    "            j = random.randint(0, self.num_samples - 1)\n",
    "            while i == j:\n",
    "                j = random.randint(0, self.num_samples - 1)\n",
    "        elif self.pairs:\n",
    "            i, j = self.pairs[idx]\n",
    "        else:\n",
    "             raise IndexError(\"Dataset not configured correctly for pair generation or is empty.\")\n",
    "\n",
    "        # Process audio to get input_values for Wav2Vec2\n",
    "        # For real usage, self.audio_input_list[i] might be a path to an audio file\n",
    "        # or a pre-loaded waveform.\n",
    "        audio_i_processed = self._process_audio(self.audio_input_list[i])\n",
    "        audio_j_processed = self._process_audio(self.audio_input_list[j])\n",
    "        \n",
    "        score_i = self.intensity_scores[i]\n",
    "        score_j = self.intensity_scores[j]\n",
    "\n",
    "        if score_i > score_j:\n",
    "            P_ij = 1.0\n",
    "        elif score_i < score_j:\n",
    "            P_ij = 0.0\n",
    "        else:\n",
    "            P_ij = 0.5\n",
    "\n",
    "        return audio_i_processed, audio_j_processed, torch.tensor(P_ij, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmoRankNet(nn.Module):\n",
    "    def __init__(self, wav2vec2_model_name=\"facebook/wav2vec2-base\", \n",
    "                scoring_head_hidden_dims=[256, 128], freeze_wav2vec2=True):\n",
    "        super(EmoRankNet, self).__init__()\n",
    "        \n",
    "        # Load pre-trained Wav2Vec2 model\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(wav2vec2_model_name)\n",
    "        \n",
    "        if freeze_wav2vec2:\n",
    "            for param in self.wav2vec2.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Determine the input dimension for the scoring head\n",
    "        # This depends on the Wav2Vec2 model's output (e.g., config.hidden_size)\n",
    "        # For \"facebook/wav2vec2-base\", config.hidden_size is 768\n",
    "        # For \"facebook/wav2vec2-large\", config.hidden_size is 1024\n",
    "        wav2vec2_output_dim = self.wav2vec2.config.hidden_size \n",
    "        \n",
    "        # Scoring head (similar to the previous IntensityScoringModel)\n",
    "        layers = []\n",
    "        current_dim = wav2vec2_output_dim\n",
    "        if scoring_head_hidden_dims:\n",
    "            for h_dim in scoring_head_hidden_dims:\n",
    "                layers.append(nn.Linear(current_dim, h_dim))\n",
    "                layers.append(nn.ReLU())\n",
    "                current_dim = h_dim\n",
    "        layers.append(nn.Linear(current_dim, 1)) # Output a single score\n",
    "        self.scoring_head = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_values (torch.Tensor): Processed audio waveforms (batch_size x sequence_length)\n",
    "            attention_mask (torch.Tensor, optional): Attention mask for padding.\n",
    "        \"\"\"\n",
    "        # Pass through Wav2Vec2\n",
    "        # The processor usually prepares the attention_mask if padding is involved\n",
    "        outputs = self.wav2vec2(input_values, attention_mask=attention_mask)\n",
    "        \n",
    "        # Use the last hidden state.\n",
    "        # We need to decide how to pool these features. Mean pooling is common.\n",
    "        # (batch_size, sequence_length, hidden_size)\n",
    "        hidden_states = outputs.last_hidden_state \n",
    "        \n",
    "        # Mean pool across the sequence dimension\n",
    "        if attention_mask is not None:\n",
    "            # Ensure attention_mask is float for division and correct broadcasting\n",
    "            # (batch_size, sequence_length) -> (batch_size, sequence_length, 1)\n",
    "            expanded_mask = attention_mask.unsqueeze(-1).float()\n",
    "            sum_hidden_states = (hidden_states * expanded_mask).sum(dim=1)\n",
    "            sum_mask = expanded_mask.sum(dim=1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9) # Avoid division by zero\n",
    "            pooled_output = sum_hidden_states / sum_mask\n",
    "        else:\n",
    "            # If no attention mask, assume no padding and mean pool\n",
    "            pooled_output = hidden_states.mean(dim=1)\n",
    "        \n",
    "        # (batch_size, hidden_size)\n",
    "        \n",
    "        # Pass pooled features through the scoring head\n",
    "        score = self.scoring_head(pooled_output) # (batch_size, 1)\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_SAMPLES = 50  # Reduced for faster dummy example\n",
    "EPOCHS = 5 # Reduced for faster dummy example\n",
    "BATCH_SIZE = 4 # Reduced for dummy data\n",
    "LEARNING_RATE = 1e-4 # May need to be smaller if fine-tuning Wav2Vec2\n",
    "SIGMA_RANKNET = 1.0\n",
    "WAV2VEC2_MODEL_NAME = \"facebook/wav2vec2-base\" # Using a smaller model for example\n",
    "TARGET_SAMPLING_RATE = 16000\n",
    "NUM_PAIRS_PER_EPOCH = NUM_SAMPLES * 2 # Reduced for faster dummy data\n",
    "\n",
    "# Dummy audio data (replace with your actual pre-processed audio data)\n",
    "# Each item should be a 1D numpy array or tensor representing a raw waveform\n",
    "# Ensure they are at the TARGET_SAMPLING_RATE\n",
    "dummy_audio_waveforms = [np.random.randn(TARGET_SAMPLING_RATE * random.randint(1,3)).astype(np.float32) for _ in range(NUM_SAMPLES)]\n",
    "\n",
    "dummy_intensity_scores = np.random.rand(NUM_SAMPLES).astype(np.float32) * 10\n",
    "\n",
    "# Load Wav2Vec2 Processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(WAV2VEC2_MODEL_NAME)\n",
    "\n",
    "# Instantiate Dataset and DataLoader\n",
    "# The PairwiseAudioDataset will now handle audio processing using the Wav2Vec2 processor\n",
    "pairwise_dataset = PairwiseAudioDataset(\n",
    "    dummy_audio_waveforms, \n",
    "    dummy_intensity_scores,\n",
    "    processor,\n",
    "    target_sampling_rate=TARGET_SAMPLING_RATE,\n",
    "    num_pairs_per_epoch=NUM_PAIRS_PER_EPOCH\n",
    ")\n",
    "\n",
    "actual_batch_size = min(BATCH_SIZE, len(pairwise_dataset))\n",
    "if actual_batch_size == 0:\n",
    "    print(\"Dataset is empty or too small for the batch size.\")\n",
    "    exit()\n",
    "\n",
    "# Custom collate_fn for padding variable length sequences from the processor\n",
    "def collate_fn_pair(batch):\n",
    "    audio_i_list, audio_j_list, p_ij_list = zip(*batch)\n",
    "    \n",
    "    # The processor should handle padding if called on a list of waveforms.\n",
    "    # However, our dataset processes them individually. So we collect and pad here.\n",
    "    # Alternatively, the dataset's _process_audio could return dicts and we use processor.pad\n",
    "    \n",
    "    processed_i = processor(list(audio_i_list), sampling_rate=TARGET_SAMPLING_RATE, return_tensors=\"pt\", padding=True)\n",
    "    processed_j = processor(list(audio_j_list), sampling_rate=TARGET_SAMPLING_RATE, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    p_ij_tensor = torch.stack(p_ij_list)\n",
    "    \n",
    "    return processed_i.input_values, processed_i.attention_mask, \\\n",
    "            processed_j.input_values, processed_j.attention_mask, \\\n",
    "            p_ij_tensor\n",
    "\n",
    "train_loader = DataLoader(pairwise_dataset, batch_size=actual_batch_size, shuffle=True, collate_fn=collate_fn_pair)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = EmoRankNet(\n",
    "    wav2vec2_model_name=WAV2VEC2_MODEL_NAME,\n",
    "    freeze_wav2vec2=True # Set to False if you want to fine-tune Wav2Vec2\n",
    ").to(device)\n",
    "\n",
    "rank_loss_fn = RankNetLoss(sigma=SIGMA_RANKNET)\n",
    "# If fine-tuning Wav2Vec2, you might need a smaller learning rate for wav2vec2 parameters\n",
    "# and a larger one for the head, or use different optimizer groups.\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "print(f\"Starting training with {len(train_loader)} batches per epoch.\")\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    for input_values_i, attention_mask_i, \\\n",
    "        input_values_j, attention_mask_j, \\\n",
    "        P_ij_target in train_loader:\n",
    "\n",
    "        input_values_i = input_values_i.to(device)\n",
    "        attention_mask_i = attention_mask_i.to(device)\n",
    "        input_values_j = input_values_j.to(device)\n",
    "        attention_mask_j = attention_mask_j.to(device)\n",
    "        P_ij_target = P_ij_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        score_i = model(input_values_i, attention_mask=attention_mask_i)\n",
    "        score_j = model(input_values_j, attention_mask=attention_mask_j)\n",
    "        \n",
    "        loss = rank_loss_fn(score_i, score_j, P_ij_target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches +=1\n",
    "    \n",
    "    if num_batches > 0:\n",
    "        avg_loss = total_loss / num_batches\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], Average Loss: {avg_loss:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch [{epoch+1}/{EPOCHS}], No data processed in this epoch.\")\n",
    "\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# After training, the model can predict a raw score for any given input feature.\n",
    "# These raw scores can then be used for ranking or, if calibrated, as intensity estimates.\n",
    "# Example of getting a score for a new sample:\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    example_feature = torch.tensor(features[0:1], dtype=torch.float32).to(device)\n",
    "    predicted_score = model(example_feature)\n",
    "    print(f\"Example feature score: {predicted_score.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
